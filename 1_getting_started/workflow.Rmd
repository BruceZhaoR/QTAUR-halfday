---
title: "quanteda workflow"
author: Kenneth Benoit
date: 15 November 2017
output:
  md_document:
    variant: markdown_github
---

This file demonstrates a basic workflow to take some pre-loaded texts and perform elementary text analysis tasks quickly. The **quanteda** package comes with a built-in set of inaugural addresses from US Presidents. We begin by loading quanteda and examining these texts. The `summary` command will output the name of each text along with the number of types, tokens and sentences contained in the text. Below we use R's indexing syntax to selectivly use the summary command on the first five texts.

```{r}
library("quanteda")

summary(data_corpus_inaugural, 10)
```


A corpus object can have document variables.  These are accessed using the `docvars()` function.

```{r}
docvars(data_corpus_inaugural) %>% head()
```

As indicated here, the "pipe" function `%>%` from the **magrittr** package is automatically supported by **quanteda**.  Here, we have extracted the data.frame of the document variables, and piped this to the `head()` function to display the first six rows of this data.


**quanteda**'s `tokens()` function can be used on a simple character vector, a vector of character vectors, or a corpus. Here are some examples:

```{r}
tokens("Today is Thursday in Canberra. It is yesterday in London.")

vec <- c(one = "This is text one", two = "This, however, is the second text")
vec
tokens(vec)
```

Consider the default arguments to the tokenize function. To remove punctuation, you should set the `removePunct` argument to be TRUE. We can combine this with the `toLower` function to get a cleaned and tokenized version of our text.

```{r}
tokens(vec, removePunct = TRUE) %>% tokens_tolower()
```

Using this function with the inaugural addresses:

```{r}
inaug_tokens <- tokens(data_corpus_inaugural) %>% tokens_tolower()
inaug_tokens[2]
```

Once each text has been split into words, we can use the `dfm()` constructor function to create a matrix of counts of the occurrences of each word in each document:

```{r}
inaug_dfm <- dfm(inaug_tokens)
trimmed_inaug_dfm <- dfm_trim(inaug_dfm, min_docfreq = 5, min_count = 10)
weighted_trimmed_dfm <- dfm_weight(trimmed_inaug_dfm, type = "tfidf")
head(weighted_trimmed_dfm)
```

Using pipes, we could have simplified this easily:
```{r}
inaug_tokens %>% 
  dfm() %>%
  dfm_trim(min_docfreq = 5, min_count = 10) %>%
  dfm_weight(type = "tfidf") %>%
  head()
```


Note that `dfm()` works on a variety of object types, including character vectors, corpus objects, and tokenized text objects.  This gives the user maximum flexibility and power, while also making it easy to achieve similar results by going directly from texts to a document-by-feature matrix.

To see what objects for which any particular *method* (function) is defined, you can use the `methods()` function:
```{r}
methods(dfm)
```

Likewise, you can also figure out what methods are defined for any given *class* of object, using the same function:
```{r}
methods(class = "tokens")
```

If we are interested in analysing the texts with respect to some other variables, we can create a corpus object to associate the texts with this metadata. For example, consider the last six inaugural addresses:

```{r}
summary(corpus_subset(data_corpus_inaugural, Year > 1993))
```

We can use the `docvars()` option to the `corpus()` command to record the party with which each text is associated:

```{r}
dv <- data.frame(Party = c("dem", "rep", "rep", "dem", "dem", "notreallyrep"))
txts <- texts(corpus_subset(data_corpus_inaugural, Year > 1993))
recentCorpus <- corpus(txts, docvars = dv)
summary(recentCorpus)
```

We can use this metadata to combine features across documents when creating a document-feature matrix:
```{r fig.width=10, fig.height=8, warning=FALSE}
partyDfm <- dfm(recentCorpus, groups = "Party", ignoredFeatures = stopwords("english"),                  verbose = FALSE)
plot(partyDfm, comparison = TRUE)
```
